{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lnms71on866r","executionInfo":{"status":"ok","timestamp":1760835334192,"user_tz":420,"elapsed":341,"user":{"displayName":"Angel Maravilla","userId":"05922231635495636496"}},"outputId":"2f5ee07f-163f-4ecd-9544-56dcfd4e25c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Epoch 001 | Loss: 87.06085968\n","Epoch 050 | Loss: 0.28439713\n","Epoch 100 | Loss: 0.00070914\n","Epoch 150 | Loss: 0.00000367\n","Epoch 200 | Loss: 0.00000006\n","Learned weights: [[0.99998283 1.000037  ]]\n","6 + 7 = predicted 13.0002 (actual 13.0000)\n","10 + -2.5 = predicted 7.4997 (actual 7.5000)\n","-8.3 + 1.2 = predicted -7.0998 (actual -7.1000)\n","0 + 0 = predicted 0.0000 (actual 0.0000)\n","1 + 10.0 = predicted 11.0004 (actual 11.0000)\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","\n","torch.manual_seed(0) #set rand seed same each time for debugging\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device:\", device) #check if we can use cuda \"hoepfully we can for actual car\"\n","\n","N = 10000  #set 10k pairs\n","X = (torch.rand(N, 2) * 20 - 10).to(device)  # works with -10,10 \"much better than just pos\"\n","y = X.sum(dim=1, keepdim=True)               # true sum [10k,1]\n","\n","model = nn.Linear(2, 1, bias=False).to(device) #2 inp 1 out neural nw no bias because pure add b should be 0\n","\n","loss_fn = nn.MSELoss()  #loss uses mse to see how far off we are \"punish\"\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.1) #adam optimizes weights based on gradients\n","\n","#training\n","EPOCHS = 200\n","for epoch in range(1, EPOCHS + 1):\n","    #grab predictions from model\n","    preds = model(X)\n","    loss = loss_fn(preds, y)  # compare to wha tit should be\n","\n","    # Backpropagation gradient descent\n","    optimizer.zero_grad() # reset prev gradients\n","    loss.backward()    #calculate new gradients\n","    optimizer.step()       # update weights\n","\n","    if epoch % 50 == 0 or epoch == 1:\n","        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.8f}\") #print every 50 epoch to track\n","\n","\n","print(\"Learned weights:\", model.weight.detach().cpu().numpy()) #check model\n","\n","#test\n","def predict(a, b):\n","    with torch.no_grad():  # dont track gradients for testing\n","        inp = torch.tensor([[float(a), float(b)]], device=device)\n","        out = model(inp).item()\n","    print(f\"{a} + {b} = predicted {out:.4f} (actual {a+b:.4f})\")\n","\n","predict(6, 7)\n","predict(10, -2.5)\n","predict(-8.3, 1.2)\n","predict(0, 0)\n","predict(1, 10.0)\n"]}]}